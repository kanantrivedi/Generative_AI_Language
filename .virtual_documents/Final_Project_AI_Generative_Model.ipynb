import numpy as np
import pandas as pd
import nltk
import string
import random
import matplotlib.pyplot as plt
from keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout


data = None
with open("Aliceâ€™s Adventures in Wonderland.txt", "r") as f:
    data = f.read()
data


data = data.lower()


data = "".join([char for char in data if char not in string.punctuation])
print(data)


def generate_sequence(text):
    length = 40
    sequences = list()
    for i in range(length, len(text)):
        # select sequence of tokens
        seq = text[i - length:i + 1]
        # store
        sequences.append(seq)
    print('Total Sequences: %d' % len(sequences))
    return sequences


# create sequences
sequences = generate_sequence(data)


sequences


chars = sorted(list(set(data)))
mapping = dict((c, i) for i, c in enumerate(chars))


def encode_char(seq):

    sequences = list()
    for line in seq:
        # integer encode line
        encoded_seq = [mapping[char] for char in line]
        # store
        sequences.append(encoded_seq)
    return sequences


# encode the sequences
sequences = encode_char(sequences)


mapping


sequences


from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
# vocabulary size
vocab = len(mapping)
sequences = np.array(sequences)
# create X and y
X, y = sequences[:, :-1], sequences[:, -1]
X = np.expand_dims(X, -1)
# one hot encode y
y = to_categorical(y, num_classes=vocab)
# create train and validation sets
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    test_size=0.2,
                                                    random_state=42)

print('Train shape:', X_train.shape, 'Val shape:', X_test.shape)
print('Train shape:', y_train.shape, 'Val shape:', y_test.shape)


X_train


model = Sequential()
model.add(LSTM(256, input_shape=(40, 1)))

model.add(Dense(vocab, activation='softmax'))


model.summary()


opt = Adam(learning_rate=0.01)
model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer=opt)
history = model.fit(X_train,
                    y_train,
                    validation_data=(X_test, y_test),
                    batch_size=512,
                    epochs=80)


# summarize history for accuracy
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()


def generate_text(model, mapping, seq_length, n_chars):
    # Start with a random seed
    max_len = 40
    start_index = random.randint(0, len(data) - max_len - 1)
    generated_text = data[start_index:start_index + max_len]

    in_text = generated_text
    # generate a fixed number of characters
    for _ in range(n_chars):
        # encode the characters as integersabs
        encoded = [mapping[char] for char in in_text]
        # truncate sequences to a fixed length
        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')

        encoded = np.expand_dims(encoded, axis=-1)
        # print(encoded)
        # predict character

        predict = model.predict(encoded)
        yhat = np.argmax(predict, axis=1)
        # reverse map integer to character
        out_char = ''
        for char, index in mapping.items():
            if index == yhat:
                out_char = char
                break
        # append to input
        in_text += out_char
    return in_text


generate_text(model, mapping, 40, 200)



